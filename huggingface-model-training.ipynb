{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9ddac6",
   "metadata": {},
   "source": [
    "# Introduction to training a NLP model with HuggingFace #\n",
    "\n",
    "Welcome friends. This notebook discusses the fundamentals of model training - how to prepare your data for training, the important hyperparemters that drive model performance, and why we go through all this trouble in the first place. We'll explore all this through the lens of <a href=\"https://huggingface.co/docs/transformers/tasks/question_answering#question-answering\">HuggingFace's Question answering task guide </a>. Nearly all the code in this demonstration is taken from the tutorial - this notebook adds color to the NLP terminology used and explains the programming logic in more detail.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928a9d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natepruitt/blog_projects/nlp_blog/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 5.27kB [00:00, 1.51MB/s]\n",
      "Downloading metadata: 2.36kB [00:00, 1.40MB/s]\n",
      "Downloading readme: 7.67kB [00:00, 1.07MB/s]\n",
      "Found cached dataset squad (/Users/natepruitt/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf775d01",
   "metadata": {},
   "source": [
    "We begin by loading the 'squad' dataset. HuggingFace offers a <a href=\"https://huggingface.co/docs/datasets/index\">wide-range of datasets</a> that can be loaded by simply passing their name as the argument to 'load_dataset'. Loading a dataset returns a dict-like 'Dataset' object with functions to help manipulate the data. \n",
    "\n",
    "\n",
    "\n",
    "When we build our model, we want to have a dataset that has inputs with labeled output, and a dataset with inputs but no <em>no</em> ouput. In the biz, these two datasets are referred to 'train' and 'test' respectively. The model uses the training set to build its prediction parameters and the test set to, you guessed it, test its prediction ability. \n",
    "\n",
    "For example, we will be building a question and answering model. A fully trained question and answering model receives a question and a context - the model then returns the answer found in the context. This is an important point - we are <em>not</em> building a text generation model like ChatGPT, where we can pass it only a question and expect an answer. Our model must have a context to extract an answer from. Therefore, our training dataset will have a question, context, and answer that the model uses - including the answer gives the model a target output as it calculates its prediction parameters. The test data set will include the question and context, but NO answer - we are taking off the training wheels (pun intended) and letting the model predict the output without our help. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916bb5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = squad.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae907e",
   "metadata": {},
   "source": [
    "Split the dataset into \"training\" and \"testing\" with the 'train_test_split' method on the <a href=\"https://huggingface.co/docs/datasets/v2.9.0/en/package_reference/main_classes#datasets.Dataset.train_test_split\">HuggingFace Dataset object.</a> \n",
    "\n",
    "The argument passed to the 'test_size' parameter determines what proportion of the original data should be dedicated to testing. Initial dataset size was 5000 questions, so the test set size is 1000 questions. Our model will be trained on the remaining 4000 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228db049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(squad[\"train\"][\"context\"]))\n",
    "print(len(squad[\"test\"][\"context\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74afdef",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Converting from human-readable to machine-readable ##\n",
    "Datasets for training natural language models are made up of human-readable raw text - as an example, here's the first question, context, and answer from our training set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8af202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:'How much did Yao Ming donate?'\n",
      "Context:'By May 14, the Ministry of Civil Affairs stated that 10.7 billion yuan (approximately US$1.5 billion) had been donated by the Chinese public. Houston Rockets center Yao Ming, one of the country's most popular sports icons, gave $214,000 and $71,000 to the Red Cross Society of China. The association has also collected a total of $26 million in donations so far. Other multinational firms located in China have also announced large amounts of donations.'\n",
      "Answer: '$214,000 and $71,000'\n"
     ]
    }
   ],
   "source": [
    "# 'question' is a list of questions. \n",
    "print(f\"Question:'{squad['train']['question'][0]}'\")\n",
    "print(f\"Context:'{squad['train']['context'][0]}'\")\n",
    "print(f\"Answer: '{squad['train']['answers'][0]['text'][0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e0b14",
   "metadata": {},
   "source": [
    "Transformer models cannot process raw text. A transformer model (or any natural language model, really) needs inputs to be numerical - the underpinnings of these AI models is math -  extremely complicated math, but math none the less. How do we transform the above sentence into numbers?\n",
    "\n",
    "### The Tokenzier ###\n",
    "\n",
    "Tokenizing (tokenizing refers to the complete process of segmenting a sentence into words / phrases and assigning a numerical id to each word / phrase) the question and context returns a dictionary like object with keys input_ids, attention_mask, and offset_mapping. \n",
    "\n",
    "A tokenizer is an object that accepts raw text sequences (sentences) as input and outputs a machine-readable version of that sequence. The machince-readable version will be a sequence of 'ids' that won't mean a lick to you or I but reads like Shakespearean poetry to our model. \n",
    "\n",
    "Initialize a tokenizer using the <a href=\"https://huggingface.co/docs/datasets/v2.9.0/en/package_reference/main_classes#datasets.Dataset.train_test_split\">HuggingFace 'AutoTokenizer' </a> class. Pass the name of the model as an argument to the 'from_pretrained' method - it should be the same name as the model you'll eventually train your dataset on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa8b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized question: ['how', 'much', 'did', 'yao', 'ming', 'donate', '?']\n"
     ]
    }
   ],
   "source": [
    "# Import and initialize tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# Tokenize the first question\n",
    "tokenized_question = tokenizer.tokenize(squad['train']['question'][0])\n",
    "print(f\"Tokenized question: {tokenized_question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff766cda",
   "metadata": {},
   "source": [
    "The 'tokenize' method returns an array of <strong>character tokens</strong>, but the model won't understand them in this format either. To transform these tokens into what are called \"ids\", call the function <a href=\"https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.convert_tokens_to_ids\">'convert_tokens_to_ids'</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e43de76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids: [2129, 2172, 2106, 23711, 11861, 21357, 1029]\n"
     ]
    }
   ],
   "source": [
    "# transform character tokens into ids\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized_question)\n",
    "print(f\"Input ids: {input_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f2b1b8",
   "metadata": {},
   "source": [
    "To reduce the process into one function call, run <a href=\"https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode\"> the encode method</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c1e5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2129, 2172, 2106, 23711, 11861, 21357, 1029, 102]\n"
     ]
    }
   ],
   "source": [
    "encoded_question = tokenizer.encode(squad['train']['question'][0])\n",
    "print(encoded_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a149c3b",
   "metadata": {},
   "source": [
    "Astute readers will notice the output sequence of 'encode'  call is different than the output of 'convert_tokens_to_ids(tokenize(text))'. Specifically, the sequence output by 'encode' has one additional token at the beginning and end of the sequence. \"Decoding\" (converting back to a string from a sequence of ids) reveals the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7dfe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded from input ids: how much did yao ming donate?\n",
      "Decoded from character tokens: [CLS] how much did yao ming donate? [SEP]\n"
     ]
    }
   ],
   "source": [
    "decoded_output_convert_tokens = tokenizer.decode(input_ids)\n",
    "decoded_output_encoded_tokens = tokenizer.decode(encoded_question)\n",
    "\n",
    "print(f\"Decoded from input ids: {decoded_output_convert_tokens}\")\n",
    "print(f\"Decoded from character tokens: {decoded_output_encoded_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7989c",
   "metadata": {},
   "source": [
    "Aha! They mystery tokens are 'CLS' and 'SEP'; they are reffered to as 'special tokens. Common in BERT derived models, 'CLS' stands for 'classification' and is placed at the beginning of input sequences. It signals to the model that the sequence is represented as a single vector (as opposed to matrix or other data structure). This helps the model make predictions, as it knows to base its prediction on the entire 'classification' sequence. \n",
    "\n",
    "'SEP' stands for seperator and exists to distinguish text within a sequence. While not relevant for the above one sentence example, it becomes crucial later on when tokenizing the questions and context together. Remember, the model needs both a 'question' and 'context' in order to make a prediction. Ultimately, each input our model receives as training data will be a <em>combined</em> vector of the question and corresponding context. More on that later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd931d",
   "metadata": {},
   "source": [
    "### Tokenizing questions and contexts ###\n",
    "\n",
    "Let's begin preprocessing the data by tokenizing all the questions and context. The <a href=\"https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__\">'tokenizer'</a> method accepts a multitude of parameters to customize its outputs. \n",
    "\n",
    "The first two parameters are <strong>text</strong> and <strong>text pair</strong>. Each is a list of strings the tokenizer will transform into a sequence.\n",
    "\n",
    "<strong>max_length</strong> sets the maximum length for each output sequence. Any token sequence longer than this will be truncated.\n",
    "\n",
    "<strong>return_offsets_mapping</strong> set to True instructs the tokenizer to include a datatable of token offset mappings in the output. Offset mapping will be explained in more detail later. \n",
    "\n",
    "<strong>padding</strong> informs the model how much to pad each sequence of input ids. A model expects each sequence to be the same length. Of course, it is unrealistic for each question and context string to be the exact same number of words. To reconcile this, a tokenizer will add (\"pad\") each sentence with a however many white space tokens are necessary to ensure consistent length.\n",
    "\n",
    "The code below generates the 'inputs' variable; a dict-like object that stores the transformed raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa2bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [q.strip() for q in squad[\"train\"][\"question\"]]\n",
    "context = [c.strip() for c in squad['train']['context']]\n",
    "inputs = tokenizer(\n",
    "        questions,\n",
    "        context,\n",
    "        max_length=384,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20e1f241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the keys in the input dictionary-like 'BatchEncoding' class.\n",
    "len(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb0bd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of dict-like object 'input': dict_keys(['input_ids', 'attention_mask', 'offset_mapping'])\n",
      "\n",
      "\n",
      "Number of rows in input_ids table: 4000\n",
      "\n",
      "\n",
      "Number of columns in input_ids table: 384\n"
     ]
    }
   ],
   "source": [
    "print(f\"Keys of dict-like object 'input': {inputs.keys()}\")\n",
    "print('\\n')\n",
    "print(f\"Number of rows in input_ids table: {len(inputs['input_ids'])}\")\n",
    "print('\\n')\n",
    "print(f\"Number of columns in input_ids table: {len(inputs['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e024a3",
   "metadata": {},
   "source": [
    "#### The 'inputs' variable ####\n",
    "\n",
    "The tokenizer objective is to return a table of 'input_ids', which for most transformer models is the only <a href=\"https://huggingface.co/transformers/v3.1.0/glossary.html#input-ids\"> required parameter </a>\n",
    "\n",
    "In the cell above are important characterstics of the 'inputs' variable. There are three keys in this dict-like object, and each key retrieves an array of arrays, which is best thought of as a datatable. Looking specifically at the datatable at key 'input_ids', each row represents a sequence of character tokens - each column being character token at that position in the sequence. For example, the value at row 1, column 2 would be the input id (numerical representation) of the character token from sequence 2, position 3 of our sample dataset (rows and columns are zero indexed). The datatable has 4000 rows (one for each example in our training data). Each row is a pairing of the question and associated context. Since training a question answering model requires both a quesiton and context, the tokenizer combines them into one sequence. In the tokenizer class call, the 'question' and 'context' are passed in seperately - the function handles joining the sequence. The input ids table has 384 columns - 384 because that is the value passed to the <strong>max_length</strong> argument in the tokenizer class call. For each question/context combined sequence that does not reach 384 character tokens, the tokenizer appends white space to the end of the sequence. \n",
    "\n",
    "As an example, below is the first row of the input ids table, along with its 'decoded', human-readable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fecf4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row in input ids table: [101, 2129, 2172, 2106, 23711, 11861, 21357, 1029, 102, 2011, 2089, 2403, 1010, 1996, 3757, 1997, 2942, 3821, 3090, 2008, 2184, 1012, 1021, 4551, 11237, 1006, 3155, 2149, 1002, 1015, 1012, 1019, 4551, 1007, 2018, 2042, 6955, 2011, 1996, 2822, 2270, 1012, 5395, 12496, 2415, 23711, 11861, 1010, 2028, 1997, 1996, 2406, 1005, 1055, 2087, 2759, 2998, 18407, 1010, 2435, 1002, 19936, 1010, 2199, 1998, 1002, 6390, 1010, 2199, 2000, 1996, 2417, 2892, 2554, 1997, 2859, 1012, 1996, 2523, 2038, 2036, 5067, 1037, 2561, 1997, 1002, 2656, 2454, 1999, 11440, 2061, 2521, 1012, 2060, 20584, 9786, 2284, 1999, 2859, 2031, 2036, 2623, 2312, 8310, 1997, 11440, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "Firt row in input ids table (decoded): [CLS] how much did yao ming donate? [SEP] by may 14, the ministry of civil affairs stated that 10. 7 billion yuan ( approximately us $ 1. 5 billion ) had been donated by the chinese public. houston rockets center yao ming, one of the country's most popular sports icons, gave $ 214, 000 and $ 71, 000 to the red cross society of china. the association has also collected a total of $ 26 million in donations so far. other multinational firms located in china have also announced large amounts of donations. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(f\"First row in input ids table: {inputs['input_ids'][0]}\")\n",
    "print('\\n')\n",
    "print(f\"Firt row in input ids table (decoded): {tokenizer.decode(inputs['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404da55",
   "metadata": {},
   "source": [
    "The decoded sequence includes [SEP] and [PAD] tokens - we discussed [SEP] tokens earlier but this example makes their importance more apparent - notice how they mark the split between question and context, and designate the end of the context, and where the [PAD] tokens begin. The [PAD] tokens are tokens added by the tokenizer to reach the 384 length requirement - these [PAD] tokens are denominated as 0 in the input id table. \n",
    "\n",
    "The tokenization process is more complicated than our example suggests. For example, how do we toeknize sentences with special characters and symbols? What about capitalization? For information on the inner workings of tokenization, HuggingFace https://www.youtube.com/watch?v=Yffk5aydLzg has a introduction video that makes a nice jumping off point for a trip down the rabbit hole. For the purposes of this demonstration, it is enough to understand that at a high level a tokenizer converts text sequences (sentences) into segmented text sequences and finally into a sequence of numerical ids. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e36a23",
   "metadata": {},
   "source": [
    "#### Attention! Attention! Read all about masks and offset mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780056a7",
   "metadata": {},
   "source": [
    "The 'attention_mask' key retrieves a datatable of boolean values that marks tokens important for prediction. The table is 4000 rows by 384 columns - the same dimmensions of the input ids table. Each cell in the attention mask table refers to the equivalent cell in the input ids table. If you want to know if the token at row i, column j is important for prediction, you check the attention mask table at row i, column j. Generally, the tokens marked unimportant for prediction are the PAD tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6a58d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input id at row 0, column 5: 11861\n",
      "Attention mask at row 0, column 5: 1\n",
      "Input id at row 0, column 0: 0\n",
      "Attention mask at row 0, column 380: 0\n"
     ]
    }
   ],
   "source": [
    "## Example of an important token\n",
    "print(f\"Input id at row 0, column 5: {inputs['input_ids'][0][5]}\")\n",
    "print(f\"Attention mask at row 0, column 5: {inputs['attention_mask'][0][5]}\")\n",
    "# The '1' we print out translates to 'Yes, this id IS important for making predictions'\n",
    "\n",
    "## Example of unimportant token\n",
    "print(f\"Input id at row 0, column 0: {inputs['input_ids'][0][380]}\")\n",
    "print(f\"Attention mask at row 0, column 380: {inputs['attention_mask'][0][380]}\")\n",
    "\n",
    "# The '0' we print out from the attention mask table translates to 'No, this id is NOT relevant for making predictions'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494dd44d",
   "metadata": {},
   "source": [
    "Offset mapping is a matrix of tuples. Each tuple in the matrix maps back to a character token. The first value in the tuple is the starting position for that character token; the second value is the closing position (non-inclusive - if the character token ends at position 4, then the second value of the tuple would be 5). In this context \"position\" refers to the character numerical order in the original sentence. The character \"r\" in the sentence \"Here comes Sally\" would have a position of 2, since the first position is set as 0 (the \"H\" character). If the above sentence was split into [\"Here\", \"comes\", \"Sally\"], then the accompanying offset mapping would be something like [(0,5), (6, 11), (12, 17)] - the segment \"Here\" begins at position 0 and ends at position 5 (remember, <italic> non-inclusive </italic>), \"comes\" begins at 6 and ends at 11, and \"Sally\" begins at 12 and ends at 17. Offset mapping is returned by the tokenzier when the 'return_offsets_mapping' argument is set to true - so why does our particular use case require it? We'll explain it through the context of our next data pre-processing step; extracting the answer tokens.\n",
    "\n",
    "Each cell in the input_ids and offset_mapping table / matrix is mapped to a character token. The id at input_ids[i][j] and the offset mapping at offset_mapping[i][j] correspond to the <em>same</em> character token. It is important to understand that input_ids, attention_mask, and offset mapping all have the same dimmensions (4000 x 384) because each cell is referring to the same underlying character token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e422d4",
   "metadata": {},
   "source": [
    "#### Final Preparations - Calculating the Answer tokens\n",
    "To help train the model, we'll pass in each answer corresponding to a question-context pair. The training dataset provides the text and 'starting position' of the answers - the character position within the context where the answer word / phrase begins.\n",
    "\n",
    "Text-based answers are nice for us humans, but the machine demands numbers. Instead of going through a long process of tokenzing the answers, lets extract them from the tokenized context sequence. We'll use a function from <a href=\"https://huggingface.co/docs/transformers/tasks/question_answering#preprocess\">HuggingFace </a> to handle the logic.\n",
    "\n",
    "We are given the location of the answer within the context, and we need to map the answer to its corresponding ids. How? With the help of the offset mapping matrix. This matrix maps the starting and ending character positions (from the original sequence) of each character token. Run the below code to compare the first cell in the offset_mapping and input_ids matrices compared with the tokenized word example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c35e583b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset mapping array for first context entry \n",
      "\n",
      "[(0, 2), (3, 6), (7, 9), (9, 10), (11, 14), (15, 23), (24, 26), (27, 32), (33, 40), (41, 47), (48, 52), (53, 55), (55, 56), (56, 57), (58, 65), (66, 70), (71, 72), (72, 85), (86, 88), (88, 89), (89, 90), (90, 91), (91, 92), (93, 100), (100, 101), (102, 105), (106, 110), (111, 118), (119, 121), (122, 125), (126, 133), (134, 140), (140, 141), (142, 149), (150, 157), (158, 164), (165, 168), (169, 173), (173, 174), (175, 178), (179, 181), (182, 185), (186, 193), (193, 194), (194, 195), (196, 200), (201, 208), (209, 215), (216, 221), (221, 222), (223, 227), (228, 229), (229, 232), (232, 233), (233, 236), (237, 240), (241, 242), (242, 244), (244, 245), (245, 248), (249, 251), (252, 255), (256, 259), (260, 265), (266, 273), (274, 276), (277, 282), (282, 283), (284, 287), (288, 299), (300, 303), (304, 308), (309, 318), (319, 320), (321, 326), (327, 329), (330, 331), (331, 333), (334, 341), (342, 344), (345, 354), (355, 357), (358, 361), (361, 362), (363, 368), (369, 382), (383, 388), (389, 396), (397, 399), (400, 405), (406, 410), (411, 415), (416, 425), (426, 431), (432, 439), (440, 442), (443, 452), (452, 453)]\n",
      "\n",
      "Tokenized context. Note how each offest mapping tuple spans the length of the corresponding character token. \n",
      "\n",
      "['by', 'may', '14', ',', 'the', 'ministry', 'of', 'civil', 'affairs', 'stated', 'that', '10', '.', '7', 'billion', 'yuan', '(', 'approximately', 'us', '$', '1', '.', '5', 'billion', ')', 'had', 'been', 'donated', 'by', 'the', 'chinese', 'public', '.', 'houston', 'rockets', 'center', 'yao', 'ming', ',', 'one', 'of', 'the', 'country', \"'\", 's', 'most', 'popular', 'sports', 'icons', ',', 'gave', '$', '214', ',', '000', 'and', '$', '71', ',', '000', 'to', 'the', 'red', 'cross', 'society', 'of', 'china', '.', 'the', 'association', 'has', 'also', 'collected', 'a', 'total', 'of', '$', '26', 'million', 'in', 'donations', 'so', 'far', '.', 'other', 'multinational', 'firms', 'located', 'in', 'china', 'have', 'also', 'announced', 'large', 'amounts', 'of', 'donations', '.']\n",
      "\n",
      "The first tuple in the offset mapping context array is (0, 2). This means the first token in the context\n",
      "\n",
      "starts at position 0 and spans to position 1 for a length of 2 characters, because when indexing an array\n",
      "\n",
      "the ending value is non-inclusive. Notice that the first token, by, is the exact length of characters.\n",
      "\n",
      "You can check the other tokens with the same process.\n"
     ]
    }
   ],
   "source": [
    "example_context = squad['train']['context'][0]\n",
    "example_question = squad['train']['question'][0]\n",
    "tokenized_context = tokenizer.tokenize(example_context)\n",
    "\n",
    "## Length of tokenized example question\n",
    "example_question_tokenized_length = len(tokenizer.tokenize(example_question))\n",
    "\n",
    "# Length of tokenized example context\n",
    "example_context_tokenized_length = len(tokenizer.tokenize(example_context))\n",
    "\n",
    "# Rows in the input ids matrix are question and context combined, with the question first. \n",
    "# Therefore, our context will start at the index equal to the length of the tokenized question plus 2. \n",
    "# We add two to account for the [CLS] and [SEP] special tokens. \n",
    "context_start_position_index = example_question_tokenized_length + 2\n",
    "context_end_position_index = context_start_position_index + example_context_tokenized_length\n",
    "\n",
    "# Offset mapping for example context\n",
    "offset_mapping_context = inputs['offset_mapping'][0][context_start_position_index:context_end_position_index]\n",
    "# input ids for context tokens\n",
    "input_ids_context = inputs['input_ids'][0][context_start_position_index:context_end_position_index]\n",
    "\n",
    "\n",
    "print('Offset mapping array for first context entry \\n')\n",
    "print(offset_mapping_context)\n",
    "print('\\nTokenized context. Note how each offest mapping tuple spans the length of the corresponding character token. \\n')\n",
    "print(tokenized_context)\n",
    "\n",
    "print(f\"\\nThe first tuple in the offset mapping context array is {offset_mapping_context[0]}. This means the first token in the context\\n\")\n",
    "print(f\"starts at position {offset_mapping_context[0][0]} and spans to position {offset_mapping_context[0][1] - 1} for a length of {offset_mapping_context[0][1] - offset_mapping_context[0][0]} characters, because when indexing an array\\n\")\n",
    "print(f\"the ending value is non-inclusive. Notice that the first token, {tokenized_context[0]}, is the exact length of characters.\\n\")\n",
    "print(f\"You can check the other tokens with the same process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703748d2",
   "metadata": {},
   "source": [
    "#### Tokens and Offset mapping ####\n",
    "We know offset mapping provides the starting and ending position of tokens. We know we can extract the starting position of the answer from the training set, AND the length of the answer. With these two pieces of information, we can identify the answer numerical ids in any row of the input_ids matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ce2d03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much did Yao Ming donate?\n",
      "Context: By May 14, the Ministry of Civil Affairs stated that 10.7 billion yuan (approximately US$1.5 billion) had been donated by the Chinese public. Houston Rockets center Yao Ming, one of the country's most popular sports icons, gave $214,000 and $71,000 to the Red Cross Society of China. The association has also collected a total of $26 million in donations so far. Other multinational firms located in China have also announced large amounts of donations.\n",
      "Answer: $214,000 and $71,000. Starting position: 228\n",
      "Number of characters in the answer: 20\n",
      "The answer can be found in the context, between character positions 228 and 248\n"
     ]
    }
   ],
   "source": [
    "answer_starting_position = squad['train']['answers'][0]['answer_start'][0]\n",
    "num_char_in_answer = len(squad['train']['answers'][0]['text'][0])\n",
    "answer_ending_position = answer_starting_position + num_char_in_answer\n",
    "# Print the question and context\n",
    "print(f'Question: {example_question}')\n",
    "print(f'Context: {example_context}')\n",
    "# Print the answer and starting position (within context). \n",
    "print(f\"Answer: {squad['train']['answers'][0]['text'][0]}. Starting position: {answer_starting_position}\")\n",
    "\n",
    "# Print the number of characters in the answer\n",
    "print(f\"Number of characters in the answer: {num_char_in_answer}\")\n",
    "\n",
    "# Answer ending position (within context).\n",
    "print(f\"The answer can be found in the context, between character positions {answer_starting_position} and {answer_ending_position}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d315f479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can now extract the token answers from the context using the indices returned via looping through the first row of offset_mapping: $ 214, 000 and $ 71, 000\n"
     ]
    }
   ],
   "source": [
    "# Loop through offset mapping and find indexes that match the character start and end positions\n",
    "start_index = 0\n",
    "end_index = 0\n",
    "for index, (start, end) in enumerate(offset_mapping_context):\n",
    "    if start == answer_starting_position:\n",
    "        start_index = index\n",
    "    if end == answer_ending_position:\n",
    "        # 1 added to account for array slicing being non-inclusive\n",
    "        end_index = index + 1\n",
    "        \n",
    "# Remember, we need to adjust the offset mapping indices because we calculated it on the context alone, but the input id rows include the question AND context.\n",
    "decoded_answer_from_input_ids = tokenizer.decode(inputs[\"input_ids\"][0][start_index + context_start_position_index: end_index + context_start_position_index])\n",
    "        \n",
    "print(f\"We can now extract the token answers from the context using the indices returned via looping through the first row of offset_mapping: {decoded_answer_from_input_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b720487",
   "metadata": {},
   "source": [
    "### Pre-processing the entire dataset ###\n",
    "\n",
    "Tokenizing the question and context, converting it to numerical ids, deriving indexes of answers with offset mapping - all this work on a single example. We need a function that applies these processing steps to the entire dataset. We borrowed the following function from <a href=\"https://huggingface.co/docs/transformers/tasks/question_answering#preprocess\">HuggingFace</a>. \n",
    "\n",
    "There is quite a bit of logic embedded in this single function. HuggingFace included comments to aid understanding, and I've added a few of my own to provide even more clarity. Before diving in, here are the high level processes the function executes.\n",
    "\n",
    " 1. Tokenize the questions and context of passed in 'dataset' argument\n",
    " 2. Using the returned offset_mapping matrix, iterate over each start, end position tuple.\n",
    " 3. For each tuple, derive the start and end position of the answers by first calculating the start / end position of the <em>context</em>\n",
    " 4. Append the start position input_ids index to the 'start_position' array. Do the same for end position input_ids index and the 'end_position' array \n",
    " 5. Include the 'start_positions' and 'end_positions' in the 'inputs' dict-like object, then return the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fefc8b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset):\n",
    "    questions = [q.strip() for q in dataset[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        dataset[\"context\"],\n",
    "        max_length=384,\n",
    "        \n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = dataset[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        # sequence_ids categorizes tokens as part of the 'question' or 'context'. Each row in the input_ids\n",
    "        # matrix contains ids for the tokenzied 'question' and 'context'. Ids alone can't determine\n",
    "        # whether a token is part of the question or context. The sequence ids function call returns\n",
    "        # an array of 0, 1, and None - 0 marking a token / input id as 'question', 1 for 'context', and\n",
    "        # None for special tokens like CLS, SEP, and PAD. \n",
    "        # sequence_ids variable will resemble [None, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        # Knowing that '1' marks a token as part of the 'context', we increment\n",
    "        # the idx variable until we know the start and end position (or index) of the context tokens \n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        # By \"full inside\", we mean words of the answer lie outside the context sentence.\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            # Increment the index counter variable 'idx' until we find the\n",
    "            # index where the answer begins by checking the first value\n",
    "            # of each offset mapping.\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    # 'start_position' and 'end_position' are arrays added to the input object\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a0a9e1",
   "metadata": {},
   "source": [
    "Apply the the function over the whole dataset with the <a href='https://huggingface.co/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.map'>'map' function. </a>\n",
    "\n",
    "Returning the dictionary-like 'inputs' variable in the mapping function adds the 'inputs' key-value pairs to the 'squad' dataset object. Since the function is called on a <a href='https://huggingface.co/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.DatasetDict' > DatasetDict object </a> (squad became a DatasetDict object when we split it into 'test' and 'train' sets), HuggingFace knows to apply the function to *both* 'test' and 'train' keys in the 'squad' DatasetDict object. \n",
    "\n",
    "Both 'test' and 'train' will be passed as the argument to the 'dataset' argument - the processing function will run independently on each.\n",
    "\n",
    "The 'remove_columns' parameter tells map to drop columns from our database. By passing in the column names of the original dataset (id, title, context, question, answers) we remove fields that the model won't be able to process. That leaves us with a clean table of <em> only </em> model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2d363c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.32ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.43ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d5355",
   "metadata": {},
   "source": [
    "Check function results by decoding the tokens within the interval created by the start and end positions of the first answer. The decoded value derived from the 'input_ids' data table should match the 'text' from the original squad dataset 'answer' table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3e2468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ 214, 000 and $ 71,\n",
      "['$214,000 and $71,000']\n"
     ]
    }
   ],
   "source": [
    "### Run function with 'map', then observe first row.\n",
    "answer_index = 0\n",
    "\n",
    "answer_start_position = tokenized_squad['train']['start_positions'][answer_index]\n",
    "answer_end_position = tokenized_squad['train']['end_positions'][answer_index]\n",
    "\n",
    "decoded_answer = tokenizer.decode(tokenized_squad['train']['input_ids'][answer_index][answer_start_position: answer_end_position])\n",
    "squad_dataset_answer = squad['train']['answers'][answer_index]['text']\n",
    "\n",
    "print(decoded_answer)\n",
    "print(squad_dataset_answer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b84de25",
   "metadata": {},
   "source": [
    "#### Data Collator ####\n",
    "Not to be confused with collector, the data collactor conducts final pre-processing format changes before the inputs are passed into the model. The primary responsibility of the data collator is to batch inputs together. Similar to the tokenizer, the data collator receives inputs and re-formats the data into a structure more favorable to the model. The data collator standardizes vector length with padding characters. \n",
    "\n",
    "Batching refers to the \"bundling\" of model inputs together before passing to the model. With batching, the model can \"process\" (i.e. adjust model parameters) the inputs in parallel. Batching reduces how much computer memory the model needs to store the data - instead of holding the entire dataset in memory, it can hold only the current batch. This leaves memory open for the CPU to utilize while it processes the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45bb51b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16412df7",
   "metadata": {},
   "source": [
    "### At Long Last - Training ###\n",
    "\n",
    "Finally, after enough pre-processing to make our heads spin, we are ready to train our model. Lets import our pre-trained model, instantiate a 'training_args' and 'trainer' instance, then call the 'train' function. \n",
    "\n",
    "Quickly, here is the explanation for each argument passed to the <a href=\"https://huggingface.co/docs/transformers/v4.27.1/en/main_classes/trainer#transformers.TrainingArguments\"> TrainingArguments class</a> (once again shamelessly stealing the configuration from <a href=\"https://huggingface.co/docs/transformers/tasks/question_answering#train\"> HuggingFace's tutorial </a>). \n",
    "\n",
    "<strong>output_dir</strong>: The only required argument, this is where the HuggingFace Trainer class saves the trained model. \n",
    "\n",
    "<strong>evaluation_strategy</strong>: When the model should \"evaluate\" itself. Specifics of model evaluation are out of scope, but broadly a model has a numerical representation (passed by a human) on what a correct output is, and the parameters are tweaked to get as close (numerically) to that output as possible. By measuring the difference between model and correct output, we can evaluate the models ability. Different metrics exist for measuring accuracy, with the <a href=\"https://en.wikipedia.org/wiki/Loss_function\">loss function</a> being the standard for regression functions. \n",
    "\n",
    "We set this value equal to 'epoch', letting the model know it should evaluate itself every epoch. An 'epoch' is one full processing of the <em>entire</em> dataset. Theoretically, as the model continues to iterate over the training set, its outputs should improve, tweaking its parameters to get closer and closer to the desired outcome. \n",
    "\n",
    "<strong>learning_rate</strong>: Learning rate influences the models step size. Step size measures the rate at which a model updates parameters during training. A higher learning rate will result in a larger step size, increasing the magnitude of the parameter rate of change. The learning rate is exactly what is sounds like - the speed at which a model \"learns\". In model terms, \"learning\" describes the process of adjusting the parameters in order to get closer to optimization. A higher learning rate decrease the time to reach optimization, but puts the model at risk of \"overfitting\" - the phenomenon where the model parameters are overly optimized for the training data, therfore peforming poorly on new data.\n",
    "\n",
    "<strong>per_device_train_batch_size, per_device_eval_batch_size</strong>: Both values correspond to our \"batch\" size, or how many example inputs we pass to the model at one time. \n",
    "\n",
    "<strong>num_train_epochs</strong>: Number of epoch cycles to perform. Remember, an epoch corresponds to training the model on the entire dataset. \n",
    "\n",
    "<strong>weight_decay</strong>: Weight decay is a \"penalty\" placed on the output of the lost function in order to reduce overfitting. A model adjusts its parameters based on the output of a loss function - by manipulating the loss function output, we reduce the magnitude of the parameter adjustment. The higher the weight_decay value, the less chance of overfitting. Of course, there is an inherent trade off to this approach - too much weight decay and the model risks being being undertrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e703078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_qa_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264cd793",
   "metadata": {},
   "source": [
    "Finally, training begins. Grab a coffee - you might wait awhile for the training to finish.\n",
    "\n",
    "Why, you may be asking if you've read this far, did we go through all this trouble to train a <em>pretrained</em> model? The pretrained model we are using - <a href=\"https://huggingface.co/distilbert-base-uncased#training-data\">distil-bert-uncased</a> - was already trained on over <a href=\"https://huggingface.co/distilbert-base-uncased#training-data\">11,000 unpublished books AND Wikipedia</a>. What difference will it make to train on a additional 4000 examples? Here are the major reasons: \n",
    "\n",
    " - The pre-trained model was built on raw, unlabeled text data. Labeling examples with correct output, as we've done with the 'start_position' and 'end_position' array, helps the model fine tune the parameters\n",
    "  - NLP models can solve a variety of problems - sentiment analysis, sequence prediction, question answering, etc. Pretrained models are general - they are not designed to solve a specific problem. We want to build a solution for question and answering - the out of the box model would be able to serve our purpose, but the output will be better if trained on question / answering designed inputs.\n",
    "  - Businesses have unique jargin and context that a general model will not understand. If a business wants to implement an NLP solution into an existing product, they'll want to train the model on industry contextualized examples. For example, a onine commerce store developing a chat bot would want to train their model on examples of common questions from real customers.\n",
    "\n",
    "Pretrained models are valuable since they've already been trained on the general rules of the language (via raw text) which saves significant time and energy in the training process. Admittedly, 4000 examples processed 3 times will not significantly improve the models outputs - but the reasons for training a pre-trained model remain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26263d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natepruitt/blog_projects/nlp_blog/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n",
      "  Number of trainable parameters = 66364418\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 1:56:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.134887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.536200</td>\n",
       "      <td>1.686889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.536200</td>\n",
       "      <td>1.649944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to my_awesome_qa_model/checkpoint-500\n",
      "Configuration saved in my_awesome_qa_model/checkpoint-500/config.json\n",
      "Model weights saved in my_awesome_qa_model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_qa_model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_qa_model/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=2.1230705159505208, metrics={'train_runtime': 6989.4713, 'train_samples_per_second': 1.717, 'train_steps_per_second': 0.107, 'total_flos': 1175877900288000.0, 'train_loss': 2.1230705159505208, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train your model!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da16901",
   "metadata": {},
   "source": [
    "### Next Steps ###\n",
    "Congratulations! You've made it through this extensive breakdown of a pre-existing HuggingFace tutorial. We've introduced the tokenizing process, explained subject vocabulary like 'epoch' and 'offset mapping', and touched briefly on why we go through all this pre-training trouble in the first place. I encourage anyone who enjoyed the article to begin exploring model evaluation. We glossed over it in this article, but an understanding of different evaluation functions and tecniques is crucial to building successful (and professional) models. Happy coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455139c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
